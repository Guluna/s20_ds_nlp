{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: In this homework you will briefly tag a few documents for person references. You will then compare your tags to a set of tags produced by myself and possibly other people. For the purposes of the submission you need only submit the comparison information with the set I've tagged.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1:\n",
    "\n",
    "Tag data. You'll want to sign up at https://dataturks.com/ . There you will want to create an account and setup a new project for tagging.\n",
    "\n",
    "Once you've created an account you'll want to create a new dataset. (+ button on the left hand side.)\n",
    "Select the Document Annotation option under Text annotations.\n",
    "    Provide a dataset name (you can use whatever)\n",
    "    for the list of entities just input Person\n",
    "    for the tagging instructions provide whatever you would like\n",
    "    Then hit submit\n",
    "    \n",
    "You will then hit the \"Upload raw data\" button. Here, go into the homework data directory and upload the zip file which has all the documents inside of it.\n",
    "\n",
    "You can then tag each document. Simply highlight the piece of text that you think is a person. When you've identified all of them for a single document hit \"move to done\". Use the guidelines contained in the next cell.\n",
    "\n",
    "When you finish tagging all 5 documents, go into the project you've tagged and hit the options button in the top right. You will then hit \"download\". You will want to have the \"complete items\" and \"json\" options selected. Hit download and you will then have a json file for all of your documents.\n",
    "\n",
    "(If this is confusing, please send me an email. I can try to come up with screen shots if necessary.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation guidelines:\n",
    "    These are from the ACE Guidelines for Person:\n",
    "    \n",
    "3.1 Persons (PER) \n",
    "    Each distinct person or set of people mentioned in a document refers to an entity of type Person. For example, people may be specified by name (“John Smith”), occupation (“the butcher”), family relation (“dad”), pronoun (“he”), etc., or by some combination of these. Dead people and human remains are to be recorded as entities of type Person. So are fictional human characters appearing in movies, TV, books, plays, etc. \n",
    "\n",
    "There are a number of words that are ambiguous as to their referent. For example, nouns, which normally refer to animals or non-humans, can be used to describe people. If it is clear to the annotator that the noun refers to a person in a given context, it should be marked as a Person entity. \n",
    "\n",
    "Examples: \n",
    "He is [a real turkey]\n",
    "[The political cat of the year]\n",
    "She’s known as [the brain of the family] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2:\n",
    "\n",
    "Look through the below code.\n",
    "\n",
    "You will see that the compare annotations method needs to be implemented. It should report back the number of matches and non-matching annotations\n",
    "\n",
    "Implement the appropriate code to calculate these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# method to read annotation file\n",
    "def annotation_processor(annotation_file):\n",
    "    annotation_array = []\n",
    "\n",
    "    # here we need to be careful and process each line of the annotation file separately\n",
    "    read_annotation = open(annotation_file)\n",
    "    for line in read_annotation:\n",
    "        data = json.loads(line)\n",
    "        annotation_array.append(data)\n",
    "\n",
    "    # here we return an array of the individual annotations\n",
    "    return annotation_array\n",
    "    \n",
    "# calling the annotation processor function\n",
    "annotation_processor('/s20_ds_nlp/homeworks/homework_3/annotated_data/annotated.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will create two objects to store the reference annotations and your own annotations\n",
    "reference_annotations = annotation_processor('/s20_ds_nlp/homeworks/homework_3/annotated_data/annotated.json')\n",
    "\n",
    "# here I am just putting the same file in... if I do this I would expect a perfect match\n",
    "my_annotations = annotation_processor('/s20_ds_nlp/homeworks/homework_3/annotated_data/annotated.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will primarily focus on implementing this compare_annotations method\n",
    "def compare_annotations(reference_annotations, my_annotations):\n",
    "    num_matches = 0\n",
    "    num_non_matches = 0\n",
    "    # you will need to implement this method\n",
    "    # The annotations you will get in the parameters are arrays of objects like this:\n",
    "    # {'label': ['Person'],\n",
    "    # 'points': [{'start': 85,\n",
    "    #  'end': 116,\n",
    "    #  'text': 'A group of students and teachers'}]}\n",
    "    # You will need to compare the annoatations between the reference and your own\n",
    "    # You will look to see if you have annotations that match when the start and end values are the same,\n",
    "    # that is the character offset is correct\n",
    "    \n",
    "    print(\"success\")\n",
    "    \n",
    "    return num_matches, num_non_matches\n",
    "    \n",
    "\n",
    "def compare_annotation_files(reference_annotation_array, my_annotation_array):\n",
    "    num_annotations_in_reference = 0\n",
    "    num_annotations_in_mine = 0\n",
    "    \n",
    "    # we want this method to calculate these two numbers\n",
    "    # num_matches should simply count all the cases where we \n",
    "    num_matches = 0\n",
    "    # num_non_matches should simply count those cases where an annotation only occurs in one file but not both\n",
    "    num_non_matches = 0\n",
    "    \n",
    "    # OPTIONAL if you want to be more precise you can look for annotations where this a partial overalp\n",
    "    num_partial_match = 0\n",
    "    \n",
    "    for annotation in reference_annotation_array:\n",
    "        for my_annotation in my_annotation_array:\n",
    "            # here we need to do some things to ensure that the documents we are comparing are identical\n",
    "            if (annotation[\"content\"] == my_annotation[\"content\"]):\n",
    "                temp_num_matches, temp_num_non_matches = compare_annotations(annotation[\"annotation\"], my_annotation[\"annotation\"])\n",
    "        \n",
    "        # implement the sum of the temp_num_matches to the num_matches\n",
    "        \n",
    "    return num_matches, num_non_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compare_annotation_files(reference_annotations, my_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3:\n",
    "\n",
    "With the numbers now available, try to determine the Cohen's Kappa for this dataset. (You can calculate this by hand if you prefer). Assume that the Probability of random agreement is 0.3.\n",
    "\n",
    "Note, with this the proportionate agreement would be the number of matches divided by the number of matches + the number of non-matches.\n",
    "\n",
    "The rest of the formula should be as discussed in class.\n",
    "\n",
    "Report the numbers you calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:\n",
    "\n",
    "You will want to write a short report of 1 or 2 paragraphs. In it you should describe:\n",
    "    What kind of differences there are between your annotations and the ones I have provided.\n",
    "        Look through the different annotations and suggest where I might have been mistaken in identifying people\n",
    "    Explain if you think tagging persons are difficult. Do you think the guidelines should be improved?\n",
    "    A brief explanation of the Cohen's Kappa you calculated. Do you think it might be high or low? Does it report anything useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5:\n",
    "\n",
    "Now you will compare the output of two extractors over a small dataset of news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are just using two different spacy models.\n",
    "# you will need to ensure that you have both models installed\n",
    "model_1 = spacy.load(\"en_core_web_sm\")\n",
    "model_2 = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = \"/s20_ds_nlp/homeworks/homework_3/news_data/\"\n",
    "\n",
    "\n",
    "####\n",
    "# Notice: We are reusing code from class notes... remember these kind of building blocks\n",
    "####\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(join(directory, f))\n",
    "        print(file_text)\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "# here we will generate the list that contains all the files and their contents\n",
    "text_corpus = read_directory_files(dir_base)\n",
    "print(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract entities\n",
    "def get_entities(document_text, model):\n",
    "    analyzed_doc = model(document_text)\n",
    "    # here we are just limiting to a small set of entity types\n",
    "    entities = [entity for entity in analyzed_doc.ents if entity.label_ in [\"PER\", \"ORG\", \"LOC\", \"GPE\"]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entities_from_document(reference_entities, test_entities):\n",
    "    print(reference_entities)\n",
    "    print(test_entities)\n",
    "    # here we need to calculate how different the reference and test entity sets are\n",
    "    # Since we treat the reference entity set as the ground truth we are trying to find \n",
    "    # how many of the same entities are returned in the test entity set\n",
    "    \n",
    "    # So we need to identify the correctly identified entities that are also in the \n",
    "    # test entity set and also if entities are not retrieved\n",
    "    # we also want to count the number of entities that are in the test entity set that\n",
    "    # are not in the reference entity set as well\n",
    "    \n",
    "    # in this case we will only concern ourselves with exact matches of entities\n",
    "    # thus, a match is the same portion of text and the same entity type\n",
    "    # hint: this is easy if you try to use normal comparisons\n",
    "    \n",
    "    correct_identified_entities = 0 \n",
    "    # count the number of items in the test set \n",
    "    # that are also in the reference set\n",
    "    \n",
    "    correct_unidentified_entities = 0\n",
    "    # count the number of items that are in the reference set \n",
    "    # that are not in the test set\n",
    "    \n",
    "    spurious_identified_entites = 0\n",
    "    # count the number of items in the test set that are not in \n",
    "    # the reference set\n",
    "    \n",
    "    \n",
    "    # one way you could do this is to iterate over the test entity list and see if each\n",
    "    # item is in the reference entity set\n",
    "    # so if an item in the test set is in the reference then you would increment the \n",
    "    # correct_identified_entities number\n",
    "    \n",
    "    ###\n",
    "    # Your code goes here\n",
    "    ###\n",
    "    \n",
    "    return correct_identified_entities, correct_unidentified_entities, spurious_identified_entites\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_correct_identified_entities = 0\n",
    "overall_correct_unidentified_entities = 0\n",
    "overall_spurious_identified_entites = 0\n",
    "\n",
    "for document in text_corpus:\n",
    "    # below you will see that entities_1 is from model_1\n",
    "    # you can make a decision about which model output will be the reference output\n",
    "    entities_1 = get_entities(document[\"content\"], model_1)\n",
    "    entities_2 = get_entities(document[\"content\"], model_2)\n",
    "    correct_identified_entities, correct_unidentified_entities, spurious_identified_entites = compare_entities_from_document(entities_1, entities_2)\n",
    "    \n",
    "    # increment the overall variables\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that you are outside the loop, determine the following values\n",
    "precision = # determine which set of numbers above needed to calculate\n",
    "recall = # determine which set of numbers above needed to calculate\n",
    "\n",
    "# what is the overall precision and recall?\n",
    "# does this change if you change the reference model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: \n",
    "Now you might want to speculate on which model appears to work better. Write a 1 paragraph brief on which model you think works better. Since we aren't using a normal evaluation set, feel free to speculate as you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Credit: \n",
    "\n",
    "- You can add a function that will help to identify which documents appear to be the most different in terms of their extracted entities\n",
    "- You can add a function that will tell which entity types exhibit the greatest difference in extraction between models\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
